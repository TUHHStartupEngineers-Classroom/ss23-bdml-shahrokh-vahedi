[
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html",
    "href": "content/01_journal/01_ml01_ml_fund.html",
    "title": "01 Machine Learning Fundamentals",
    "section": "",
    "text": "Your organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Salesforce CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nWe will be using stock prices in this analysis. We come up with a method to classify companies based on how their stocks trade using their daily stock returns (percentage movement from one day to the next). This analysis will help our organization determine which companies are related to each other (competitors and have similar attributes).\nWe can analyze the stock prices using what we’ve learned in the unsupervised learning tools including K-Means and UMAP. We will use a combination of kmeans() to find groups and umap() to visualize similarity of daily stock returns."
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.1 Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "5.1 Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat we first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\] First, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %>% glimpse()\n\n#> Rows: 1,225,765\n#> Columns: 8\n#> $ symbol   <chr> \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT~\n#> $ date     <date> 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, ~\n#> $ open     <dbl> 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53~\n#> $ high     <dbl> 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68~\n#> $ low      <dbl> 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01~\n#> $ close    <dbl> 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09~\n#> $ volume   <dbl> 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5~\n#> $ adjusted <dbl> 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1~\n\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>%\n  \n  select(symbol, date, adjusted) %>%\n  \n  filter(date >= ymd(\"2018-01-01\")) %>%\n  \n  group_by(symbol) %>%\n  mutate(lag_1 = lag(adjusted)) %>%\n  ungroup() %>%\n  \n  filter(!is.na(lag_1)) %>%\n  \n  mutate(diff = adjusted - lag_1) %>%\n  mutate(pct_return = diff / lag_1) %>%\n  \n  select(symbol, date, pct_return)\n\n# Apply your data transformation skills\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\nsaveRDS(sp_500_daily_returns_tbl, file = \"data01/sp_500_daily_returns_tbl.RDS\")"
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-2---convert-to-user-item-format",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-2---convert-to-user-item-format",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.2 Step 2 - Convert to User-Item Format",
    "text": "5.2 Step 2 - Convert to User-Item Format\n\nstock_date_matrix_tbl <- readRDS(\"data01/sp_500_daily_returns_tbl.rds\") %>%\n  spread(key = date, value = pct_return, fill = 0)\n\nstock_date_matrix_tbl"
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-3---perform-k-means-clustering",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-3---perform-k-means-clustering",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.3 Step 3 - Perform K-Means Clustering",
    "text": "5.3 Step 3 - Perform K-Means Clustering\n\nkmeans_obj <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  kmeans(centers = 4, nstart = 20)\n\nkmeans_obj %>% glance()"
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-4---find-the-optimal-value-of-k",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-4---find-the-optimal-value-of-k",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.4 Step 4 - Find the optimal value of K",
    "text": "5.4 Step 4 - Find the optimal value of K\n\n# Lets use `purrr` to iterate over many values of \"k\" using the `centers` argument. \nkmeans_mapper <- function(center = 3) {\n  stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    kmeans(centers = center, nstart = 20)\n}\n\n# Apply the `kmeans_mapper()` and `glance()` functions iteratively using `purrr`.\n\nk_means_mapped_tbl <- tibble(centers = 1:30) %>%\n  mutate(k_means = centers %>% map(kmeans_mapper)) %>%\n  mutate(glance  = k_means %>% map(glance))\n\n#Output: k_means_mapped_tbl\nk_means_mapped_tbl\n\n\n\n  \n\n\n\nScree Plot\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nk_means_mapped_tbl %>%\n  unnest(glance) %>%\n  ggplot(aes(centers, tot.withinss)) +\n  geom_point(color = \"#3e502c\") +\n  geom_line(color = \"#502c50\") +\n  labs(title = \"Scree Plot\") +\n  theme_tq()\n\n\n\n\n\n\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-5---apply-umap",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-5---apply-umap",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.5 Step 5 - Apply UMAP",
    "text": "5.5 Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\n\n# let's plot the `UMAP` 2D visualization to help us investigate cluster assignments. \numap_results <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  umap()\n\n# Next, we want to combine the `layout` from the `umap_results` with the `symbol` column from the `stock_date_matrix_tbl`. \numap_results_tbl <- umap_results$layout %>%\n  as_tibble() %>%\n  bind_cols(stock_date_matrix_tbl %>% select(symbol)) \n\n#> Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#> `.name_repair` is omitted as of tibble 2.0.0.\n#> i Using compatibility `.name_repair`.\n\numap_results_tbl\n\n\n\n  \n\n\n# Finally, let's make a quick visualization of the `umap_results_tbl`.\n\numap_results_tbl %>%\n  ggplot(aes(V1, V2)) +\n  geom_point(alpha = 0.5, color = \"#2c3e50\") +\n  theme_tq() +\n  labs(title = \"UMAP Projection\")\n\n\n\n\n\n\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "content/01_journal/01_ml01_ml_fund.html#step-6---combine-k-means-and-umap",
    "href": "content/01_journal/01_ml01_ml_fund.html#step-6---combine-k-means-and-umap",
    "title": "01 Machine Learning Fundamentals",
    "section": "\n5.6 Step 6 - Combine K-Means and UMAP",
    "text": "5.6 Step 6 - Combine K-Means and UMAP\n\n# Next, we combine the K-Means clusters and the UMAP 2D representation\n# First, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. \n\nk_means_mapped_tbl <- read_rds(\"data01/k_means_mapped_tbl.rds\")\numap_results_tbl   <- read_rds(\"data01/umap_results_tbl.rds\")\n\nk_means_obj <- k_means_mapped_tbl %>%\n  filter(centers == 10) %>%\n  pull(k_means) %>%\n  pluck(1)\n\n# Next, we'll combine the clusters from the `k_means_obj` with the `umap_results_tbl`.\n\numap_kmeans_results_tbl <- k_means_obj %>% \n  augment(stock_date_matrix_tbl) %>%\n  select(symbol, .cluster) %>%\n  left_join(umap_results_tbl, by = \"symbol\") %>%\n  left_join(sp_500_index_tbl %>% select(symbol, company, sector),\n            by = \"symbol\")\n\n# Plot the K-Means and UMAP results.\n\numap_kmeans_results_tbl %>%\n  ggplot(aes(V1, V2, color = .cluster)) +\n  geom_point(alpha = 0.5) +\n  theme_tq() +\n  scale_color_tq()\n\n\n\n\n\n\n\nCongratulations! We are done with the 1st challenge!"
  },
  {
    "objectID": "content/01_journal/02_ml02_sup_ML.html",
    "href": "content/01_journal/02_ml02_sup_ML.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_ml03_h2oI.html",
    "href": "content/01_journal/03_ml03_h2oI.html",
    "title": "03 Automated Machine Learning with H2O (I)",
    "section": "",
    "text": "1 Business case study\n\n# Libraries \nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(rsample)\n\n# Load Data data definitions\nemployee_attrition_tbl <- read_csv(\"data03/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\")\n\n#> Rows: 1470 Columns: 35\n#> -- Column specification --------------------------------------------------------\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npath_data_definitions <- \"data03/data_definitions.xlsx\"\ndefinitions_raw_tbl   <- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)\n\n#> New names:\n#> * `` -> `...1`\n#> * `` -> `...2`\n\nemployee_attrition_tbl\n\n\n\n  \n\n\n\n\n# Business & Data Understanding: Department and Job Role\n\n# Data subset\ndept_job_role_tbl <- employee_attrition_tbl %>%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\n\ndept_job_role_tbl %>%\n\n  group_by(Attrition) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n# Attrition by department\ndept_job_role_tbl %>%\n\n  # Block 1\n  group_by(Department, Attrition) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n\n  # Block 2: Caution: It's easy to inadvertently miss grouping when creating counts & percents within groups\n  group_by(Department) %>%\n  mutate(pct = n / sum(n))\n\n#> `summarise()` has grouped output by 'Department'. You can override using the\n#> `.groups` argument.\n\n\n\n\n  \n\n\n# Attrition by job role\ndept_job_role_tbl %>%\n\n  # Block 1\n  group_by(Department, JobRole, Attrition) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n\n  # Block 2\n  group_by(Department, JobRole) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() %>%\n\n  # Block 3\n  filter(Attrition %in% \"Yes\")\n\n#> `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#> using the `.groups` argument.\n\n\n\n\n  \n\n\n\n\n# Develop KPI\n\ndept_job_role_tbl %>%\n\n  # Block 1\n  group_by(Department, JobRole, Attrition) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n\n  # Block 2\n  group_by(Department, JobRole) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() %>%\n\n  # Block 3\n  filter(Attrition %in% \"Yes\") %>%\n  arrange(desc(pct)) %>%\n  mutate(\n    above_industry_avg = case_when(\n      pct > 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"\n    )\n  )\n\n#> `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#> using the `.groups` argument.\n\n\n\n\n  \n\n\n\n\n# Function to calculate attrition cost\ncalculate_attrition_cost <- function(\n\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50\n\n) {\n\n  # Direct Costs\n  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n\n  # Lost Productivity Costs\n  productivity_cost <- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open\n\n  # Estimated Turnover Per Employee\n  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction\n\n  # Total Cost of Employee Turnover\n  total_cost <- n * cost_per_employee\n\n  return(total_cost)\n\n}\n\ncalculate_attrition_cost()\n\n#> [1] 78483.33\n\ncalculate_attrition_cost(200)\n\n#> [1] 15696667\n\n\n\n# Use this\n# Function to convert counts to percentages. \ncount_to_pct <- function(data, ..., col = n) {\n\n  # capture the dots\n  grouping_vars_expr <- quos(...)\n  col_expr <- enquo(col)\n\n  ret <- data %>%\n    group_by(!!! grouping_vars_expr) %>%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%\n    ungroup()\n\n  return(ret)\n\n}\n\n# This is way shorter and more flexibel\ndept_job_role_tbl %>%\n  count(JobRole, Attrition) %>%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %>%\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole)  \n\n\n\n  \n\n\n\n\nassess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {\n\n  attrition_col_expr <- enquo(attrition_col)\n\n  data %>%\n  \n    # Use parenthesis () to give tidy eval evaluation priority\n    filter((!! attrition_col_expr) %in% attrition_value) %>%\n    arrange(desc(pct)) %>%\n    mutate(\n      # Function inputs in numeric format (e.g. baseline_pct = 0.088 don't require tidy eval)\n      above_industry_avg = case_when(\n        pct > baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n    )\n\n}\n\ndept_job_role_tbl %>%\n\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole) %>%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  )\n\n\n\n  \n\n\n\n\ndept_job_role_tbl %>%\n\n  group_by(Department, JobRole, Attrition) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n\n  group_by(Department, JobRole) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() %>%\n\n  filter(Attrition %in% \"Yes\") %>%\n  arrange(desc(pct)) %>%\n  mutate(\n    above_industry_avg = case_when(\n      pct > 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"\n    )\n  ) %>%\n\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  )\n\n#> `summarise()` has grouped output by 'Department', 'JobRole'. You can override\n#> using the `.groups` argument.\n\n\n\n\n  \n\n\n\n\ndept_job_role_tbl %>%\n\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole) %>%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %>%\n\n  # Data Manipulation\n  mutate(name = str_c(Department, JobRole, sep = \": \") %>% as_factor()) %>%\n\n  # Check levels\n  # pull(name) %>%\n  # levels()\n\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %>%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %>%\n\n  #Plotting\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\",\n       x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Function to plot attrition\nplot_attrition <- function(data, \n                           ..., \n                           .value,\n                           fct_reorder = TRUE,\n                           fct_rev     = FALSE,\n                           include_lbl = TRUE,\n                           color       = \"#2dc6d6\",\n                           units       = c(\"0\", \"K\", \"M\")) {\n\n  ### Inputs\n  group_vars_expr   <- quos(...)\n  \n  # If the user does not supply anything, \n  # this takes the first column of the supplied data\n  if (length(group_vars_expr) == 0) {\n    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))\n    }\n\n  value_expr <- enquo(.value)\n\n  units_val  <- switch(units[[1]],\n                       \"M\" = 1e6,\n                       \"K\" = 1e3,\n                       \"0\" = 1)\n  if (units[[1]] == \"0\") units <- \"\"\n\n  # Data Manipulation\n  # This is a so called Function Factory (a function that produces a function)\n  usd <- scales::dollar_format(prefix = \"$\", largest_with_cents = 1e3)\n\n  # Create the axis labels and values for the plot\n  data_manipulated <- data %>%\n    mutate(name = str_c(!!! group_vars_expr, sep = \": \") %>% as_factor()) %>%\n    mutate(value_text = str_c(usd(!! value_expr / units_val),\n                              units[[1]], sep = \"\"))\n\n  \n  # Order the labels on the y-axis according to the input\n  if (fct_reorder) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%\n      arrange(name)\n  }\n\n  if (fct_rev) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_rev(name)) %>%\n      arrange(name)\n  }\n\n  # Visualization\n  g <- data_manipulated %>%\n\n        # \"name\" is a column name generated by our function internally as part of the data manipulation task\n        ggplot(aes(x = (!! value_expr), y = name)) +\n        geom_segment(aes(xend = 0, yend = name), color = color) +\n        geom_point(aes(size = !! value_expr), color = color) +\n        scale_x_continuous(labels = scales::dollar) +\n        scale_size(range = c(3, 5)) +\n        theme(legend.position = \"none\")\n\n  # Plot labels if TRUE\n  if (include_lbl) {\n    g <- g +\n      geom_label(aes(label = value_text, size = !! value_expr),\n                 hjust = \"inward\", color = color)\n  }\n\n  return(g)\n\n}\n\n\ndept_job_role_tbl %>%\n\n  # Select columnns\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole) %>%\n  \n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %>%\n\n  # Select columnns\n  plot_attrition(Department, JobRole, .value = cost_of_attrition,\n                 units = \"M\") +\n  labs(\n    title = \"Estimated Cost of Attrition by Job Role\",\n    x = \"Cost of Attrition\",\n    subtitle = \"Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost\"\n  )\n\n\n\n\n\n\n\n\n# Step 1: Data Summarization -----\n\nskim(employee_attrition_tbl)\n\n\nData summary\n\n\nName\nemployee_attrition_tbl\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAttrition\n0\n1\n2\n3\n0\n2\n0\n\n\nBusinessTravel\n0\n1\n10\n17\n0\n3\n0\n\n\nDepartment\n0\n1\n5\n22\n0\n3\n0\n\n\nEducationField\n0\n1\n5\n16\n0\n6\n0\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nJobRole\n0\n1\n7\n25\n0\n9\n0\n\n\nMaritalStatus\n0\n1\n6\n8\n0\n3\n0\n\n\nOver18\n0\n1\n1\n1\n0\n1\n0\n\n\nOverTime\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nAge\n0\n1\n36.92\n9.14\n18\n30.00\n36.0\n43.00\n60\n▂▇▇▃▂\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n465.00\n802.0\n1157.00\n1499\n▇▇▇▇▇\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n2.00\n7.0\n14.00\n29\n▇▅▂▂▂\n\n\nEducation\n0\n1\n2.91\n1.02\n1\n2.00\n3.0\n4.00\n5\n▂▃▇▆▁\n\n\nEmployeeCount\n0\n1\n1.00\n0.00\n1\n1.00\n1.0\n1.00\n1\n▁▁▇▁▁\n\n\nEmployeeNumber\n0\n1\n1024.87\n602.02\n1\n491.25\n1020.5\n1555.75\n2068\n▇▇▇▇▇\n\n\nEnvironmentSatisfaction\n0\n1\n2.72\n1.09\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n48.00\n66.0\n83.75\n100\n▇▇▇▇▇\n\n\nJobInvolvement\n0\n1\n2.73\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▁\n\n\nJobLevel\n0\n1\n2.06\n1.11\n1\n1.00\n2.0\n3.00\n5\n▇▇▃▂▁\n\n\nJobSatisfaction\n0\n1\n2.73\n1.10\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n2911.00\n4919.0\n8379.00\n19999\n▇▅▂▁▂\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n8047.00\n14235.5\n20461.50\n26999\n▇▇▇▇▇\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n1.00\n2.0\n4.00\n9\n▇▃▂▂▁\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n12.00\n14.0\n18.00\n25\n▇▅▃▂▁\n\n\nPerformanceRating\n0\n1\n3.15\n0.36\n3\n3.00\n3.0\n3.00\n4\n▇▁▁▁▂\n\n\nRelationshipSatisfaction\n0\n1\n2.71\n1.08\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nStandardHours\n0\n1\n80.00\n0.00\n80\n80.00\n80.0\n80.00\n80\n▁▁▇▁▁\n\n\nStockOptionLevel\n0\n1\n0.79\n0.85\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▂▁\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n6.00\n10.0\n15.00\n40\n▇▇▂▁▁\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n2.00\n3.0\n3.00\n6\n▂▇▇▂▃\n\n\nWorkLifeBalance\n0\n1\n2.76\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▂\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n3.00\n5.0\n9.00\n40\n▇▂▁▁▁\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n2.00\n3.0\n7.00\n18\n▇▃▂▁▁\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n0.00\n1.0\n3.00\n15\n▇▁▁▁▁\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n2.00\n3.0\n7.00\n17\n▇▂▅▁▁\n\n\n\n\n# Character Data Type\nemployee_attrition_tbl %>%\n    select_if(is.character) %>%\n    glimpse()\n\n#> Rows: 1,470\n#> Columns: 9\n#> $ Attrition      <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",~\n#> $ BusinessTravel <chr> \"Travel_Rarely\", \"Travel_Frequently\", \"Travel_Rarely\", ~\n#> $ Department     <chr> \"Sales\", \"Research & Development\", \"Research & Developm~\n#> $ EducationField <chr> \"Life Sciences\", \"Life Sciences\", \"Other\", \"Life Scienc~\n#> $ Gender         <chr> \"Female\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fe~\n#> $ JobRole        <chr> \"Sales Executive\", \"Research Scientist\", \"Laboratory Te~\n#> $ MaritalStatus  <chr> \"Single\", \"Married\", \"Single\", \"Married\", \"Married\", \"S~\n#> $ Over18         <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", ~\n#> $ OverTime       <chr> \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No~\n\n# Get \"levels\"\nemployee_attrition_tbl %>%\n    select_if(is.character) %>%\n    map(unique)\n\n#> $Attrition\n#> [1] \"Yes\" \"No\" \n#> \n#> $BusinessTravel\n#> [1] \"Travel_Rarely\"     \"Travel_Frequently\" \"Non-Travel\"       \n#> \n#> $Department\n#> [1] \"Sales\"                  \"Research & Development\" \"Human Resources\"       \n#> \n#> $EducationField\n#> [1] \"Life Sciences\"    \"Other\"            \"Medical\"          \"Marketing\"       \n#> [5] \"Technical Degree\" \"Human Resources\" \n#> \n#> $Gender\n#> [1] \"Female\" \"Male\"  \n#> \n#> $JobRole\n#> [1] \"Sales Executive\"           \"Research Scientist\"       \n#> [3] \"Laboratory Technician\"     \"Manufacturing Director\"   \n#> [5] \"Healthcare Representative\" \"Manager\"                  \n#> [7] \"Sales Representative\"      \"Research Director\"        \n#> [9] \"Human Resources\"          \n#> \n#> $MaritalStatus\n#> [1] \"Single\"   \"Married\"  \"Divorced\"\n#> \n#> $Over18\n#> [1] \"Y\"\n#> \n#> $OverTime\n#> [1] \"Yes\" \"No\"\n\n# Proportions    \nemployee_attrition_tbl %>%\n    select_if(is.character) %>%\n    map(~ table(.) %>% prop.table())\n\n#> $Attrition\n#> .\n#>        No       Yes \n#> 0.8387755 0.1612245 \n#> \n#> $BusinessTravel\n#> .\n#>        Non-Travel Travel_Frequently     Travel_Rarely \n#>         0.1020408         0.1884354         0.7095238 \n#> \n#> $Department\n#> .\n#>        Human Resources Research & Development                  Sales \n#>             0.04285714             0.65374150             0.30340136 \n#> \n#> $EducationField\n#> .\n#>  Human Resources    Life Sciences        Marketing          Medical \n#>       0.01836735       0.41224490       0.10816327       0.31564626 \n#>            Other Technical Degree \n#>       0.05578231       0.08979592 \n#> \n#> $Gender\n#> .\n#> Female   Male \n#>    0.4    0.6 \n#> \n#> $JobRole\n#> .\n#> Healthcare Representative           Human Resources     Laboratory Technician \n#>                0.08911565                0.03537415                0.17619048 \n#>                   Manager    Manufacturing Director         Research Director \n#>                0.06938776                0.09863946                0.05442177 \n#>        Research Scientist           Sales Executive      Sales Representative \n#>                0.19863946                0.22176871                0.05646259 \n#> \n#> $MaritalStatus\n#> .\n#>  Divorced   Married    Single \n#> 0.2224490 0.4578231 0.3197279 \n#> \n#> $Over18\n#> .\n#> Y \n#> 1 \n#> \n#> $OverTime\n#> .\n#>        No       Yes \n#> 0.7170068 0.2829932\n\n# Numeric Data\nemployee_attrition_tbl %>%\n    select_if(is.numeric) %>%\n    map(~ unique(.) %>% length())\n\n#> $Age\n#> [1] 43\n#> \n#> $DailyRate\n#> [1] 886\n#> \n#> $DistanceFromHome\n#> [1] 29\n#> \n#> $Education\n#> [1] 5\n#> \n#> $EmployeeCount\n#> [1] 1\n#> \n#> $EmployeeNumber\n#> [1] 1470\n#> \n#> $EnvironmentSatisfaction\n#> [1] 4\n#> \n#> $HourlyRate\n#> [1] 71\n#> \n#> $JobInvolvement\n#> [1] 4\n#> \n#> $JobLevel\n#> [1] 5\n#> \n#> $JobSatisfaction\n#> [1] 4\n#> \n#> $MonthlyIncome\n#> [1] 1349\n#> \n#> $MonthlyRate\n#> [1] 1427\n#> \n#> $NumCompaniesWorked\n#> [1] 10\n#> \n#> $PercentSalaryHike\n#> [1] 15\n#> \n#> $PerformanceRating\n#> [1] 2\n#> \n#> $RelationshipSatisfaction\n#> [1] 4\n#> \n#> $StandardHours\n#> [1] 1\n#> \n#> $StockOptionLevel\n#> [1] 4\n#> \n#> $TotalWorkingYears\n#> [1] 40\n#> \n#> $TrainingTimesLastYear\n#> [1] 7\n#> \n#> $WorkLifeBalance\n#> [1] 4\n#> \n#> $YearsAtCompany\n#> [1] 37\n#> \n#> $YearsInCurrentRole\n#> [1] 19\n#> \n#> $YearsSinceLastPromotion\n#> [1] 16\n#> \n#> $YearsWithCurrManager\n#> [1] 18\n\nemployee_attrition_tbl %>%\n    select_if(is.numeric) %>%\n    map_df(~ unique(.) %>% length()) %>%\n    # Select all columns\n    pivot_longer(everything()) %>%\n    arrange(value) %>%\n    filter(value <= 10)\n\n\n\n  \n\n\n\n\n# Step 2: Data Visualization ----\n\nemployee_attrition_tbl %>%\n    select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n    ggpairs() \n\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nemployee_attrition_tbl %>%\n    select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n    ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n    \n    color_expr <- enquo(color)\n    \n    if (rlang::quo_is_null(color_expr)) {\n        \n        g <- data %>%\n            ggpairs(lower = \"blank\") \n        \n    } else {\n        \n        color_name <- quo_name(color_expr)\n        \n        g <- data %>%\n            ggpairs(mapping = aes_string(color = color_name), \n                    lower = \"blank\", legend = 1,\n                    diag = list(continuous = wrap(\"densityDiag\", \n                                                  alpha = density_alpha))) +\n            theme(legend.position = \"bottom\",\n                  text = element_text(size=8),\n                  axis.text = element_text(size = 10),\n                  axis.title = element_text(size = 10))\n    }\n    \n    return(g)\n    \n}\n\n\nemployee_attrition_tbl %>%\n    select(Attrition,  contains(\"Training\")) %>%\n    plot_ggpairs(Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> i Please use tidy evaluation idioms with `aes()`.\n#> i See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n2 Challenge\nUse your learning from descriptive features and plot_ggpairs() to further investigate the features. Run the functions above according to the features needed. Answer the following questions. Most of the time, you will only need the images from diagonal.\n\n#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Monthly Income and Attrition?\n\n\nThose that are leaving the company have a higher Monthly Income\nThat those are staying have a lower Monthly Income\nThose that are leaving have a lower Monthly Income\nIt’s difficult to deduce anything based on the visualization\n\nSolution: c\n\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition?\n\n\nThose that are leaving the company have a higher Percent Salary Hike\nThose that are staying have a lower Percent Salary Hike\nThose that are leaving have lower Percent Salary Hike\nIt’s difficult to deduce anything based on the visualization\n\nSolution: d\n\nWhat can you deduce about the interaction between Stock Option Level and Attrition?\n\n\nThose that are leaving the company have a higher stock option level\nThose that are staying have a higher stock option level\nIt’s difficult to deduce anything based on the visualization\n\nSolution: b\n\n#   4. Survey Results: Satisfaction level, WorkLifeBalance \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition?\n\n\nA higher proportion of those leaving have a low environment satisfaction level\nA higher proportion of those leaving have a high environment satisfaction level\nIt’s difficult to deduce anything based on the visualization\n\nSolution: a\n\nWhat can you deduce about the interaction between Work Life Balance and Attrition?\n\n\nThose that are leaving have higher density of 2’s and 3’s\nThose that are staying have a higher density of 2’s and 3’s\nThose that are staying have a lower density of 2’s and 3’s\nIt’s difficult to deduce anything based on the visualization\n\nSolution: b\n\n#   5. Performance Data: Job Involvment, Performance Rating\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat Can you deduce about the interaction between Job Involvement and Attrition?\n\n\nThose that are leaving have a lower density of 3’s and 4’s\nThose that are leaving have a lower density of 1’s and 2’s\nThose that are staying have a lower density of 2’s and 3’s\nIt’s difficult to deduce anything based on the visualization\n\nSolution: a\n\n#   6. Work-Life Features \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Over Time and Attrition?\n\n\nThe proportion of those leaving that are working Over Time are high compared to those that are not leaving\nThe proportion of those staying that are working Over Time are high compared to those that are not staying\n\nSolution: b\n\n#   7. Training and Education \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Training Times Last Year and Attrition?\n\n\nPeople that leave tend to have more annual trainings\nPeople that leave tend to have less annual trainings\nIt’s difficult to deduce anything based on the visualization\n\nSolution: b\n\n#   8. Time-Based Features: Years at company, years in current role\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"years\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Years At Company and Attrition?\n\n\nPeople that leave tend to have more working years at the company\nPeople that leave tend to have less working years at the company\nIt’s difficult to deduce anything based on the visualization\n\nSolution: b\n\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition?\n\n\nThose that are leaving have more years since last promotion than those that are staying\nThose that are leaving have fewer years since last promotion than those that are staying\nIt’s difficult to deduce anything based on the visualization\n\nSolution: c"
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html",
    "href": "content/01_journal/04_ml04_h2oII.html",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(recipes)\nlibrary(rsample)\n\ndataset <- read_csv(\"data04/product_backorders.csv\") %>% \n  mutate(product_backorder = went_on_backorder %>% str_to_lower() %>% str_detect(\"yes\") %>% as.numeric()) %>% select(-c(went_on_backorder))\n\n#> Rows: 19053 Columns: 23\n#> -- Column specification --------------------------------------------------------\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dataset)\n\n#> Rows: 19,053\n#> Columns: 23\n#> $ sku               <dbl> 1113121, 1113268, 1113874, 1114222, 1114823, 1115453~\n#> $ national_inv      <dbl> 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,~\n#> $ lead_time         <dbl> 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2~\n#> $ in_transit_qty    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0~\n#> $ forecast_3_month  <dbl> 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4~\n#> $ forecast_6_month  <dbl> 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, ~\n#> $ forecast_9_month  <dbl> 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, ~\n#> $ sales_1_month     <dbl> 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, ~\n#> $ sales_3_month     <dbl> 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3~\n#> $ sales_6_month     <dbl> 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4~\n#> $ sales_9_month     <dbl> 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0~\n#> $ min_bank          <dbl> 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0~\n#> $ potential_issue   <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n#> $ pieces_past_due   <dbl> 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n#> $ perf_6_month_avg  <dbl> 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00~\n#> $ perf_12_month_avg <dbl> 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95~\n#> $ local_bo_qty      <dbl> 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ deck_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n#> $ oe_constraint     <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n#> $ ppap_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No~\n#> $ stop_auto_buy     <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye~\n#> $ rev_stop          <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n#> $ product_backorder <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n\nsplit_obj<- initial_split(dataset, prop = 0.85)\ntrain_data<- training(split_obj)\ntest_data<- testing(split_obj)"
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html#specifiy-the-response-and-predictor-variables",
    "href": "content/01_journal/04_ml04_h2oII.html#specifiy-the-response-and-predictor-variables",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "\n2 Specifiy the response and predictor variables",
    "text": "2 Specifiy the response and predictor variables\n\nrecipe_obj <- recipe(product_backorder ~., data = train_data) %>% \n    step_zv(all_predictors()) %>% \n    step_dummy(all_nominal(),-all_outcomes()) %>%\n    prep()\n\nsummary(recipe_obj)\n\n\n\n  \n\n\nglimpse(bake(recipe_obj,new_data = NULL))\n\n#> Rows: 16,195\n#> Columns: 23\n#> $ sku                 <dbl> 2003094, 1446382, 1181601, 2041493, 2898957, 19291~\n#> $ national_inv        <dbl> 0, 2, 616, 10, 97, 33, 5, 8, 11, 508, 2, 6, 2, 3, ~\n#> $ lead_time           <dbl> 4, 2, 2, 8, 2, 8, 2, 2, 8, 8, 9, 8, 12, 4, 14, 8, ~\n#> $ in_transit_qty      <dbl> 0, 0, 0, 0, 27, 0, 0, 0, 0, 123, 0, 0, 0, 3, 0, 9,~\n#> $ forecast_3_month    <dbl> 11, 0, 0, 0, 273, 0, 0, 0, 0, 1734, 0, 2, 0, 0, 0,~\n#> $ forecast_6_month    <dbl> 11, 0, 0, 0, 525, 0, 0, 0, 3, 2690, 0, 8, 0, 2, 0,~\n#> $ forecast_9_month    <dbl> 11, 0, 0, 0, 735, 13, 0, 1, 12, 2920, 0, 16, 0, 3,~\n#> $ sales_1_month       <dbl> 0, 0, 1, 0, 73, 5, 0, 0, 5, 181, 0, 0, 0, 0, 0, 48~\n#> $ sales_3_month       <dbl> 0, 0, 2, 0, 224, 22, 0, 1, 9, 572, 0, 4, 0, 2, 0, ~\n#> $ sales_6_month       <dbl> 0, 0, 5, 0, 450, 38, 0, 2, 20, 1342, 0, 11, 0, 6, ~\n#> $ sales_9_month       <dbl> 0, 0, 6, 0, 682, 67, 0, 2, 24, 3138, 0, 13, 0, 7, ~\n#> $ min_bank            <dbl> 1, 0, 0, 1, 53, 5, 0, 0, 0, 209, 0, 0, 0, 0, 0, 96~\n#> $ pieces_past_due     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n#> $ perf_6_month_avg    <dbl> 0.24, 0.98, 0.99, 0.99, 0.93, 0.99, 0.99, 0.77, 0.~\n#> $ perf_12_month_avg   <dbl> 0.14, 0.95, 0.97, 0.99, 0.89, 0.99, 0.99, 0.49, 0.~\n#> $ local_bo_qty        <dbl> 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n#> $ product_backorder   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n#> $ potential_issue_Yes <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n#> $ deck_risk_Yes       <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n#> $ oe_constraint_Yes   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n#> $ ppap_risk_Yes       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,~\n#> $ stop_auto_buy_Yes   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n#> $ rev_stop_Yes        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~"
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html#run-automl-specifying-the-stopping-criterion",
    "href": "content/01_journal/04_ml04_h2oII.html#run-automl-specifying-the-stopping-criterion",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "\n3 run AutoML specifying the stopping criterion",
    "text": "3 run AutoML specifying the stopping criterion\n\nlibrary(h2o)\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 hours 29 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 16 days \n#>     H2O cluster name:           H2O_started_from_R_Shahrokh_dje900 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   0.79 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\nsplit_h2o <- h2o.splitFrame(as.h2o(train_data), ratios = c(0.85), seed = 52)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_data)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the target and predictors\ny <- \"product_backorder\"\nx <- setdiff(names(train_h2o), y)\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 140,\n  nfolds            = 5,\n  stopping_metric = \"mae\", stopping_rounds = 3,\n                        stopping_tolerance = 1e-3\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n#> 15:03:33.692: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 15:03:33.694: Stopping tolerance set by the user is < 70% of the recommended default of 0.008512565307587486, so models may take a long time to converge or may not converge at all.\n#> 15:03:33.713: AutoML: XGBoost is not available; skipping it.\n#> 15:03:33.784: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:03:33.784: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |===                                                                   |   4%\n#> 15:03:37.919: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:03:37.919: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |=============================                                         |  41%\n#> 15:04:32.205: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:04:32.205: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n#> 15:04:37.800: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:04:37.801: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n#> 15:04:49.923: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:04:49.924: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  64%\n#> 15:05:02.635: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:02.636: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n#> 15:05:15.577: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:15.578: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n#> 15:05:29.291: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:29.291: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |===========================================================           |  85%\n#> 15:05:33.50: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:33.50: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n#> 15:05:38.474: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:38.474: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |================================================================      |  92%\n#> 15:05:41.704: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:41.705: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |==================================================================    |  94%\n#> 15:05:44.940: _train param, Dropping bad and constant columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:44.940: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |====================================================================  |  97%\n#> 15:05:49.271: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:49.271: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training.\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n#> 15:05:53.444: _train param, Dropping unused columns: [potential_issue, ppap_risk, rev_stop, stop_auto_buy, deck_risk, oe_constraint]\n#> 15:05:53.444: _response param, We have detected that your response column has only 2 unique values (0/1). If you wish to train a binary model instead of a regression model, convert your target column to categorical before training."
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html#view-the-leaderboard",
    "href": "content/01_journal/04_ml04_h2oII.html#view-the-leaderboard",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "\n4 View the leaderboard",
    "text": "4 View the leaderboard\n\nautoml_models_h2o@leaderboard \n\n#>                                                  model_id      rmse        mse\n#> 1 StackedEnsemble_BestOfFamily_2_AutoML_7_20230613_150333 0.2238527 0.05011002\n#> 2    StackedEnsemble_AllModels_2_AutoML_7_20230613_150333 0.2239285 0.05014395\n#> 3    StackedEnsemble_AllModels_1_AutoML_7_20230613_150333 0.2240410 0.05019439\n#> 4 StackedEnsemble_BestOfFamily_3_AutoML_7_20230613_150333 0.2240663 0.05020570\n#> 5                          GBM_4_AutoML_7_20230613_150333 0.2280299 0.05199763\n#> 6                          GBM_3_AutoML_7_20230613_150333 0.2292833 0.05257081\n#>         mae     rmsle mean_residual_deviance\n#> 1 0.1197909 0.1563357             0.05011002\n#> 2 0.1212188 0.1570722             0.05014395\n#> 3 0.1217558 0.1572235             0.05019439\n#> 4 0.1188522 0.1561823             0.05020570\n#> 5 0.1229130 0.1580747             0.05199763\n#> 6 0.1227182 0.1587060             0.05257081\n#> \n#> [14 rows x 6 columns]\n\nautoml_models_h2o@leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2ORegressionModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_2_AutoML_7_20230613_150333 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              2/3\n#> 3      # GBM base models (used / total)              1/1\n#> 4      # DRF base models (used / total)              1/1\n#> 5      # GLM base models (used / total)              0/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2ORegressionMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.03118179\n#> RMSE:  0.1765837\n#> MAE:  0.09491372\n#> RMSLE:  0.1240345\n#> Mean Residual Deviance :  0.03118179\n#> \n#> \n#> H2ORegressionMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.05113511\n#> RMSE:  0.2261307\n#> MAE:  0.1187695\n#> RMSLE:  0.1597094\n#> Mean Residual Deviance :  0.05113511\n#> \n#> \n#> H2ORegressionMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05585049\n#> RMSE:  0.2363271\n#> MAE:  0.1281826\n#> RMSLE:  0.1660027\n#> Mean Residual Deviance :  0.05585049\n#> \n#> \n#> Cross-Validation Metrics Summary: \n#>                              mean       sd cv_1_valid cv_2_valid cv_3_valid\n#> mae                      0.128143 0.003582   0.128664   0.125750   0.133788\n#> mean_residual_deviance   0.055844 0.003179   0.055055   0.052784   0.061032\n#> mse                      0.055844 0.003179   0.055055   0.052784   0.061032\n#> null_deviance          294.516000 6.591096 298.259860 298.508330 289.537570\n#> r2                       0.476549 0.028720   0.488863   0.507356   0.432752\n#> residual_deviance      154.013980 6.272614 152.447780 147.057460 164.236470\n#> rmse                     0.236238 0.006655   0.234638   0.229749   0.247046\n#> rmsle                    0.165742 0.003994   0.165399   0.161340   0.172139\n#>                        cv_4_valid cv_5_valid\n#> mae                      0.128037   0.124477\n#> mean_residual_deviance   0.056324   0.054025\n#> mse                      0.056324   0.054025\n#> null_deviance          300.727480 285.546720\n#> r2                       0.488725   0.465051\n#> residual_deviance      153.708470 152.619700\n#> rmse                     0.237327   0.232432\n#> rmsle                    0.165924   0.163908\n\n?h2o.deeplearning\n\n#> starting httpd help server ... done\n\nextract_h2o_model_name_by_position <- function(h2o_leaderboard, n = 1, verbose = T) {\n    model_name <- h2o_leaderboard %>%\n        as.tibble() %>%\n        slice_(n) %>%\n        pull(model_id)\n    if (verbose) message(model_name)\n    return(model_name)\n}"
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html#predicting-using-leader-model",
    "href": "content/01_journal/04_ml04_h2oII.html#predicting-using-leader-model",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "\n5 Predicting using Leader Model",
    "text": "5 Predicting using Leader Model\n\nbest_model <- automl_models_h2o@leaderboard %>% \n  extract_h2o_model_name_by_position(1) %>% \n  h2o.getModel()\n\n#> Warning: `slice_()` was deprecated in dplyr 0.7.0.\n#> i Please use `slice()` instead.\n\n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> i Please use `as_tibble()` instead.\n#> i The signature and semantics have changed, see `?as_tibble`.\n\n\n#> StackedEnsemble_AllModels_1_AutoML_2_20230612_12304\n\npredictions <- h2o.predict(best_model, newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#> [1] \"environment\"\n\npredictions_tbl <- predictions %>% as_tibble()\n\nglimpse(predictions_tbl)\n\n#> Rows: 2,858\n#> Columns: 1\n#> $ predict <dbl> 0.45918184, 0.69144190, 0.08459433, 0.48058109, 0.19629379, 0.~"
  },
  {
    "objectID": "content/01_journal/04_ml04_h2oII.html#save-the-leader-model",
    "href": "content/01_journal/04_ml04_h2oII.html#save-the-leader-model",
    "title": "04 Automated Machine Learning with H2O (II)",
    "section": "\n6 Save the leader model",
    "text": "6 Save the leader model\n\nbest_model %>% h2o.saveModel(path = \"data04/model\")\n\n#> [1] \"C:\\\\Users\\\\Shahrokh\\\\Documents\\\\GitHub\\\\ss23-bdml-shahrokh-vahedi\\\\content\\\\01_journal\\\\data04\\\\model\\\\StackedEnsemble_AllModels_1_AutoML_1_20230612_05038\""
  },
  {
    "objectID": "content/01_journal/05_ml05_perf_meas.html",
    "href": "content/01_journal/05_ml05_perf_meas.html",
    "title": "05 Performance Measure",
    "section": "",
    "text": "library(tidyverse)\nlibrary(yardstick)\nlibrary(workflows)\nlibrary(tune)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(parsnip)\n\ndataset <- read_csv(\"data05/product_backorders.csv\") %>% mutate( product_backorder = went_on_backorder %>% str_to_lower() %>% str_detect(\"yes\") %>% as.numeric() ) %>% mutate(product_backorder = as.factor(product_backorder)) %>% select(-c(went_on_backorder))\n\nglimpse(dataset)\n\nsplit_obj<- initial_split(dataset, prop = 0.75)\ntrain_tbl<- training(split_obj)\ntest_tbl<- testing(split_obj)\n\n\n\nrecipe_obj <- recipe(product_backorder ~., data = train_tbl) %>% \n    step_zv(all_predictors()) %>% \n    step_dummy(all_nominal(),-all_outcomes()) %>%\n    prep()\n\nsummary(recipe_obj)\n\n\n\nlibrary(h2o)\nh2o.init()\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.75), seed = 42)\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\ny <- \"product_backorder\"\nx <- setdiff(names(train_h2o), y)\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5,\n  stopping_metric = \"auc\", stopping_rounds = 3,\n                        stopping_tolerance = 1e-2\n)\n\n\n\nautoml_models_h2o@leaderboard \n\nsummary(automl_models_h2o@leaderboard %>% as_tibble() )\n\ndata_transformed_tbl <- automl_models_h2o@leaderboard %>%\n        as_tibble() %>%\n        select(-c(rmse, mse)) %>% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n        slice(1:15) %>% \n        rownames_to_column(var = \"rowname\") %>%\n        # Visually this step will not change anything\n        # It reorders the factors under the hood\n        mutate(\n          model_id   = as_factor(model_id) %>% reorder(auc),\n          model_type = as.factor(model_type)\n          ) %>% \n          pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       ) %>% \n        mutate(model_id = paste0(rowname, \". \", model_id) %>% as_factor() %>% fct_rev())\n\ndata_transformed_tbl %>%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = 3) +\n        geom_label(aes(label = round(value, 2), hjust = \"inward\")) +\n        # Facet to break out logloss and auc\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", \"auc\"),\n             y = \"Model Postion, Model ID\", x = \"\") + \n        theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "content/01_journal/06_ml06_lime.html",
    "href": "content/01_journal/06_ml06_lime.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "Welcome to my web journal, dedicated to showcasing the fascinating outcomes of my course “Business Decisions with Machine Learning” using the powerful R programming language. As an enthusiastic learner and explorer of the world of data-driven decision making, I have embarked on a journey to uncover the potential of machine learning in shaping impactful business strategies. Through this journal, I will be sharing the results of various tasks and challenges undertaken during the course, providing valuable insights and practical applications of machine learning techniques in real-world scenarios. Join me on this exciting voyage as we dive into the realm of business decisions powered by the wonders of R programming and unleash the true potential of data-driven innovation. Let’s unlock the secrets that lie within and revolutionize the way we approach business challenges."
  }
]